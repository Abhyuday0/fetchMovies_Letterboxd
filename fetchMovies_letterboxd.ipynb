{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# import time\n",
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables\n",
    "\n",
    "url = \"https://letterboxd.com/films/popular/decade/2010s/page/\"\n",
    "page_count = 2\n",
    "soup = 0\n",
    "link = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To parse the given page\n",
    "def parse_page(url = url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    service = Service(executable_path= r\"C:\\Users\\abhyu\\Downloads\\geckodriver-v0.32.1-win64\\geckodriver.exe\")\n",
    "    driver = webdriver.Firefox(options=options, service=service)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    # print(soup.prettify)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for film in individual page\n",
    "\n",
    "def more_film_data(link=link):\n",
    "    film_soup = parse_page(link)\n",
    "    \n",
    "    actor_data = film_soup.find_all('a', {\"class\" : \"text-slug tooltip\"}, limit= 3)\n",
    "    actors = []\n",
    "    for actor in actor_data:\n",
    "        actors.append(actor.text)\n",
    "\n",
    "    director = film_soup.find('span', {\"class\" : \"prettify\"}).text\n",
    "\n",
    "    # country_or_countries = film_soup.find()\n",
    "    country_tag = film_soup.find('h3', text=re.compile(r'Country|Countries')).find_next('div')\n",
    "    country = [a.text for a in country_tag.find_all('a')]\n",
    "\n",
    "    language = film_soup.find('h3', text=re.compile(r'Original Language|Language')).find_next('div').text.strip()\n",
    "\n",
    "    rating = film_soup.find('meta', {\"name\" : \"twitter:data2\"})[\"content\"]\n",
    "    rating = float(rating.replace(' out of 5',''))\n",
    "\n",
    "    letterboxd_views = film_soup.find('li', class_=\"stat filmstat-watches\").find_next('a')['data-original-title']\n",
    "    letterboxd_views = int(''.join(filter(str.isdigit, letterboxd_views)))\n",
    "\n",
    "    #Genre\n",
    "\n",
    "    #Number of films in the franchise\n",
    "\n",
    "    #LINKS FOR OTHER SITES\n",
    "    imdb_link = film_soup.find('a', {\"data-track-action\" : \"IMDb\"})['href']\n",
    "    tmdb_link = film_soup.find('a', {\"data-track-action\" : \"TMDb\"})['href']\n",
    "\n",
    "    more_film_list = [rating, letterboxd_views, director, country, language, actors, imdb_link, tmdb_link]\n",
    "    # more_film_list.append(actors)\n",
    "\n",
    "    \n",
    "    return more_film_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of all films in a given page\n",
    "def collect_data(soup = soup):\n",
    "    # ratings = soup.find_all('li', {\"class\" : \"listitem poster-container\"}) \n",
    "    list_of_movies = soup.find_all('div', {\"data-component-class\" : \"globals.comps.FilmPosterComponent\"}) \n",
    "\n",
    "    df_page = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    #One movie at a time\n",
    "    for index, attributes in enumerate(list_of_movies):\n",
    "        # print(attributes.get('data-film-name')+', '+ attributes.get('data-film-release-year'))\n",
    "        link = 'https://letterboxd.com' + attributes.get('data-film-link')\n",
    "        row = [attributes.text, attributes.get('data-film-name'), attributes.get('data-film-release-year'), link]\n",
    "        print(attributes.get('data-film-name'))\n",
    "\n",
    "        more_film_list = more_film_data(link)\n",
    "        row.extend(more_film_list)\n",
    "        \n",
    "        # print(row)\n",
    "        df_row = pd.DataFrame(row)\n",
    "\n",
    "        df_page = pd.concat([df_page, df_row], axis=1)\n",
    "\n",
    "        #Testing\n",
    "        if index == 3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return df_page\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the pages\n",
    "def page_loop(url=url, page_count=page_count):\n",
    "\n",
    "   \n",
    "    for count in range(1, page_count):\n",
    "        url = url + str(count)\n",
    "        soup = parse_page(url)\n",
    "        df_page = collect_data(soup)\n",
    "        df = pd.concat([df, df_page])\n",
    "        # print(count)\n",
    "    return df.transpose()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "\n",
    "# def scrape_website(url, page_count):\n",
    "#     # Initialize the dataframe and the list of urls to be scraped\n",
    "#     df = pd.DataFrame()\n",
    "#     urls = [url + str(i) for i in range(1, page_count + 1)]\n",
    "\n",
    "#     # Define the maximum number of threads to be used\n",
    "#     max_threads = 10\n",
    "\n",
    "#     # Initialize a list to store the futures\n",
    "#     futures = []\n",
    "\n",
    "#     # Define the function to be executed by each thread\n",
    "#     def thread_fn(url):\n",
    "#         soup = parse_page(url)\n",
    "#         return collect_data(soup)\n",
    "\n",
    "#     # Use ThreadPoolExecutor to execute the function for each url in parallel\n",
    "#     with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "#         for url in urls:\n",
    "#             futures.append(executor.submit(thread_fn, url))\n",
    "\n",
    "#     # Concatenate the dataframes from each thread into a single dataframe\n",
    "#     for future in futures:\n",
    "#         df_page = future.result()\n",
    "#         df = pd.concat([df, df_page])\n",
    "\n",
    "#     return df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url=url, page_count=page_count):\n",
    "    df = page_loop(url, page_count)\n",
    "    # df.columns = [\"Name and Year\", \"Name\", \"Year\", \"Link\", \"Letterboxd Rating\", \"Letterboxd Views\", \"Director\", \"Country\", \"Language\", \"Actors\" ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data(url, page_count+1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# film_soup = parse_page(\"https://letterboxd.com/film/joker-2019/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "70f7196d6bc20009748e8aff01cde381ebebfbcc6f9843c14b33e0a33895b9e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
